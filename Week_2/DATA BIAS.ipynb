{
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    },
    "kernelspec": {
      "name": "python",
      "display_name": "Pyolite",
      "language": "python"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "# <u> **DATA SET BIAS** </u>\nreference and examples from https://towardsdatascience.com/survey-d4f168791e57",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## ***INTRODUCTION***\n**Machine Learning** has constantly been bewitching audiences over the past decade in many fields. The main reason is undoubtedly the successful models we have witnessed with ML in different areas like image recognition, recommendation systems, online advertisements, etc. Alexa and Siri are famous among the masses, and constant development in this technology is exponentially adding to their popularity.  \nAlongside the expansion of these applications, there has been a disturbing ascent in reports of bias based on orientation, race and different kinds of predisposition in these frameworks.  \nOne famous example can be a model used by Amazon to score candidates for employment which was discontinued after they realized it penalized women.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## ***WHAT IS DATA BIAS?***\n**Data Bias** is a sort of mistake wherein certain components of a dataset are all the more vigorously weighted as well as addressed than others. In broader sense, the output data may not be what is expected and may dicriminate with a certain group of people.  \nData bias occurs due to structural characteristics of the systems that produce the data.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## ***ORIGIN OF DATA BIAS***\nAccording to Andrew Gelman,***\"The most important aspect of a statistical analysis is not what you do with the data, it’s what data you use\"*** which makes it very clear that quality of data we use to train a model has more say in its performance than how large data set we use.  \nAny model is trained on a data set to learn how to perfom and we can never state that all big data sets on which ML/AI are based are unbiased. Any big data set is known to be biased. However this is bias is not so easy to identify and even when it is, it is hard to look for ways to eliminate this bias.Before using data to train any ML model, some authors prefer discussing the data about its origin, what data was used for and what modifications are done to the data before handling it for training which ensures better performance of th model but some developers prefer impressing others with just the complexity of their models instead of accuracy and this reduces accuracy of their models and can be harmful for humans in many ways.  ",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Data Bias can be classified on the following types:\n* *Response or Activity Bias*  \n* *Selection Bias due to Feedback Loops*\n* *Bias due to System Drift*  \n* *Omitted Variable Bias*  \n* *Societal Bias* ",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "##  ***Response or Activity Bias*** \nThis type of bias occurs due to content generated by humans like reviews E-Commerce platforms, Wikipedia Entries ,etc. The fact is only a small proportion of people contribute this content and their opinions and preferences are unlikely to reflect the opinions of the population as a whole.  \nA very simple example to understand this is of online movie reviews. We often check online ratings of movies on IMDB or other platforms but out of all the viewers, a very few viewers actually take time to review movies on these platforms and their taste may or may not match with someone else's taste. while checking somone may form a view about a movie which has average ratings without actaually watching it. This is being biased based on Response or Avtivity.  \nStatistically speaking:  \n* only 7% of users produce 50% of the posts on Facebook.\n* 4% of users produce 50% of the reviews on Amazon\n* 0.04% of Wikipedia’s registered editors (about 2000 people) produced the first version of half the entries of English Wikipedia.  \n\nThese users belong to different geographical locations and demographic groups and horror is that if Response Bias is not ressolved, then their opinions shape opinions of rest of the viewers.  Clearly, the data cannot be used to make inferences about all users.  \n\nAs one example, Crawford reports that Twitter tweets were used to study people’s behavior when Hurricane Sandy struck the US northeast. The researchers were trying to understand the behaviour of people in the worst hit areas but later they discovered that most of the data came from Manhattan (almost 2,901 miles away) and a very few tweets came from severely affected regions in New York. This is because in worst hit areas, \nHoping to understand the behavior of users in the worst hit areas, the researchers later discovered that most of the data came from Manhattan. Very few tweets came from the more severely affected regions in New York. She points out that over time, power blackouts set in, phone batteries drained even fewer tweets came from the worst hit areas.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## ***Selection Bias due to Feedback Loops***\nThis type of bias occurs when a model itself influences the generation of data that is used to train it. Users' response to items are collected which are usually based on  details of presentation like font, media, background or images and response to items to which users don't interact remain unknown. The items with most number of responses remain on top and are easily observed by other users.  This is mostly in case of ad personalization or content recommendations on streaming services.These recommendation models influence the feedback generated by users across the web and feeds it back into the system as training data again for the model which the creates feedback bias which in turn direct other user's attention to a small subset of items and record their interaction with these items by tracking their activities related to that data like clicks, views, likes, comments and scrolls. This also includes:-  \nPresenting the items in a sorted according to the views or preferences of the other users introduces **Position Bias.**  \nIf the items vary in terms of fonts and media types, it introduces **Presentation Bias**  \nIf the recommendations are presented based on feedback of other users then it can also change users’ long-term content consumption behavior and therefore homogenizing their behavior, relative to the same platform without recommended content.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## ***Bias due to System Drift***\nDrift here means change over time to the system which generates data to train the model. These changes can include even a minute change like changing the source of data or a significant one like in a way an attribute is defined or captured or any change in the algorithm that changes way of interaction between data and model. This drift can significantly reduce the performance of the model over time.  \nWe can take into account the failure of Google Flu Trends model by Google which was used to “forecast” the expected number of flu cases for a season. But in February 2013 it was observed that GFT predicted more than double the proportion of doctor visits for influenza-like illness than the Centers for Disease Control and Prevention, which bases its estimates on surveillance reports from laboratories across the United States. This was because google introduced “recommended searches” which reccomends searches based on similar searches by other users which increased magnitude of certain searches.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## ***Omitted Variable Bias***\nSometimes the data which determine the outcome is not complete or lacks some major attributes, this can affect the quality of the outcome because the outcome is not based on complete required data. It happens in systems where data generation relies on humans manually inputting the data or \nor when some attributes may not be available to include in the data set due to reasons like privacy concerns, etc. or are not detectable by the system\nTwo conditions must hold true for omitted-variable bias to exist in regression models:  \n* The omitted variable must be correlated with the dependent (target) variable.\n* The omitted variable must be correlated with one or more other explanatory (or predictor) variables.\n*for example:* A laptop manufacturer has an online chat system which its customers can use for support requests or to ask questions. The manufacturer wants to use the opportunity to cross-sell products and has developed a model to score users on how likely they are to buy additional products. The score is intended help agents working the chat system to allocate their time efficiently. When they are busy agents put in more effort (and time) trying to cross-sell to users with high-scores and less effort on users with lower scores. However, the time (and effort) expended by the agents is not recorded. Without this data it will appear that the scoring model is performing very well, whereas the time spent by agents might be much better explanation for user purchase decisions.\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## ***Societal bias***\nThese kind of basis are based on stereotypes based on gender or race and originate when system identifies these stereotypes based on articles, social media posts etc. They are a form of label bias which result in discrimination based on these stereotypes. We can consider the following *example*:  \nAmazon tried to build an AI tool to screen candidates until management discovered that it had learned to penalize women candidates. The problem is that in most companies today, technical roles are filled by men and this bias creeps into any models that use current employee data to train models.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "",
      "metadata": {}
    }
  ]
}